#!/usr/bin/env python

from __future__ import print_function

from os import environ

import boto3


def main():
    try:
        environ["AWS_ACCESS_KEY_ID"] = environ["TXAWS_INTEGRATION_AWS_ACCESS_KEY_ID"]
        environ["AWS_SECRET_ACCESS_KEY"] = environ["TXAWS_INTEGRATION_AWS_SECRET_ACCESS_KEY"]
    except KeyError as e:
        raise SystemExit("Could not clean up: {} missing from environment".format(e))
    cleanup_s3()
    cleanup_route53()


def cleanup_route53():
    route53 = boto3.client("route53")
    for zone in all_hosted_zones(route53):
        try:
            delete_hosted_zone(route53, zone)
        except Exception as e:
            print("Failed to delete Route53 hosted zone {}: {}".format(
                zone[u"Name"], e
            ))
        else:
            print("Deleted Route53 zone {}".format(zone[u"Name"]))


def all_hosted_zones(route53):
    return route53.list_hosted_zones()[u"HostedZones"]


def delete_hosted_zone(route53, zone):
    delete_all_rrsets(route53, zone)
    route53.delete_hosted_zone(Id=zone[u"Id"])


def delete_all_rrsets(route53, zone):
    rrsets = route53.list_resource_record_sets(HostedZoneId=zone[u"Id"])[u"ResourceRecordSets"]
    changes = list(
        {u"Action": u"DELETE", u"ResourceRecordSet": rrset}
        for rrset in rrsets
        if rrset[u"Type"] != u"SOA"
        and not (rrset[u"Type"] == u"NS" and rrset[u"Name"] == zone[u"Name"])
    )
    if len(changes):
        route53.change_resource_record_sets(
            HostedZoneId=zone[u"Id"],
            ChangeBatch={
                u"Comment": u"cleanup-aws",
                u"Changes": changes,
            },
        )
        print("Deleted Route53 zone {}'s {} rrsets".format(zone[u"Name"], len(rrsets)))
    else:
        print("Route53 zone {} has no rrsets to delete".format(zone[u"Name"]))


def cleanup_s3():
    s3 = boto3.client("s3")
    for bucket in all_buckets(s3):
        try:
            delete_bucket(s3, bucket)
        except Exception as e:
            print("Failed to delete S3 bucket {}: {}".format(bucket, e))
        else:
            print("Deleted S3 bucket {}".format(bucket))


def all_buckets(s3):
    # Apparently pagination isn't required for buckets - perhaps
    # because the default limit on number of buckets is so low.
    return (
        bucket[u"Name"]
        for bucket
        in s3.list_buckets()[u"Buckets"]
    )


def delete_bucket(s3, bucket):
    delete_objects(s3, bucket)
    s3.delete_bucket(Bucket=bucket)


def delete_objects(s3, bucket):
    keys = []
    for obj in list_objects(s3, bucket):
        keys.append(obj[u"Key"])
        if len(keys) == 1000:
            delete_some_objects(s3, bucket, keys)
            keys = []
    if keys:
        delete_some_objects(s3, bucket, keys)


def list_objects(s3, bucket):
    paginator = s3.get_paginator("list_objects")
    result = list(paginator.paginate(Bucket=bucket))
    return (
        content
        for page in result
        for content in page.get(u"Contents", [])
    )


def delete_some_objects(s3, bucket, keys):
    keys = list(keys)
    s3.delete_objects(
        Bucket=bucket,
        Delete={
            'Objects': list(
                {u"Key": key}
                for key in keys
            ),
        },
    )
    print("Deleted S3 object {}".format(keys))

main()
